This project implements a Retrieval-Augmented Generation (RAG) system using three different vector backends: Chroma, FAISS, and 
Redis, combined with Ollama-powered large language models (LLMs) and various embedding models. It allows users to take in PDFs from 
a ./notes/ directory, embed the content using models like nomic-embed-text, mpnet, or instructor-xl, and then perform search 
with answers based off context generated by mistral or llama. For each backend, there are two scripts, one for ingestion and one for 
search, such as faiss_ingest.py and faiss_search.py. During ingestion, users choose the embedding model and chunk size and overlap.
In this case, I applied the chunk size and overlap function to the FAISS vector database because it was the most efficient database
to me, but if you think there are other vector databases that suit your taste, you can add or call those functions. The scripts 
extract text from PDFs, split it into overlapping chunks, compute the embeddings, and store them in the selected vector database. 
During search, users enter queries, and the system retrieves the top-matching chunks and uses an LLM of the user's choice, to 
generate a response. Optimal performance was observed with a chunk size of 500 and overlap of 100, however, this might differ for 
different users. The project requires Python 3.8+ and libraries like pymupdf, faiss-cpu, redis, sentence-transformers, 
InstructorEmbedding, chromadb, and ollama, with models pulled from ollama. 
